{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
    "from xgboost import  XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMLModel:\n",
    "    def __init__(self):\n",
    "        self.priority_mapping = {'High': 2, 'Medium': 1, 'Low': 0}\n",
    "        self.priority_reverse_mapping = {2: 'High', 1: 'Medium', 0: 'Low'}\n",
    "        self.priority_encoder = LabelEncoder()\n",
    "        self.user_tasks = None\n",
    "        self.duration_features = None\n",
    "        self.priority_features = None\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def Preprocessing_Features(self, data, calendar_events, reminders, event_schedule, user_settings, workload_insights):\n",
    "        \"\"\"Enhanced feature engineering using all available datasets\"\"\"\n",
    "        features = data.copy()   \n",
    "             \n",
    "        features['Priority'] = features['Priority'].astype(str)\n",
    "        features['Status'] = features['Status'].astype(str)\n",
    "\n",
    "\n",
    "        def convert_minutes_to_datetime(minutes_str):\n",
    "            try:\n",
    "                # Split the string and convert to float\n",
    "                total_minutes, seconds = map(float, minutes_str.split(':'))\n",
    "                \n",
    "                # Convert total minutes to hours and minutes\n",
    "                hours = int(total_minutes // 60)\n",
    "                minutes = int(total_minutes % 60)\n",
    "                \n",
    "                # Combine with seconds\n",
    "                time_str = f\"{hours:02}:{minutes:02}:{int(seconds):02}\"\n",
    "                \n",
    "                # Convert to datetime\n",
    "                return pd.to_datetime(time_str, format='%H:%M:%S')\n",
    "            except:\n",
    "                # If conversion fails, return NaT\n",
    "                return pd.NaT\n",
    "            \n",
    "        features[\"Deadline\"] = features[\"Deadline\"].apply(convert_minutes_to_datetime)\n",
    "        features[\"Created At\"] = features[\"Created At\"].apply(convert_minutes_to_datetime)\n",
    "\n",
    "        #estimted duration column are in minutes\n",
    "        features[\"Estimated Duration (min)\"] = pd.to_timedelta(features[\"Estimated Duration (min)\"], unit='m')\n",
    "\n",
    "        # Extract hours, minutes, and seconds\n",
    "        features[\"Estimated Duration (min)\"] = features[\"Estimated Duration (min)\"].apply(lambda x: \n",
    "            pd.to_datetime(f\"{x.seconds // 3600:02}:{(x.seconds % 3600) // 60:02}:{x.seconds % 60:02}\", format='%H:%M:%S').time())\n",
    "        \n",
    "\n",
    "\n",
    "        def time_difference_in_minutes(deadline, created_at):\n",
    "            if deadline is None or created_at is None:\n",
    "                return None\n",
    "            \n",
    "            # Convert each time to minutes since midnight\n",
    "            deadline_minutes = deadline.hour * 60 + deadline.minute + deadline.second/60\n",
    "            created_minutes = created_at.hour * 60 + created_at.minute + created_at.second/60\n",
    "            \n",
    "            # Calculate difference\n",
    "            diff = deadline_minutes - created_minutes\n",
    "            \n",
    "            # Handle case where deadline is on the next day (crosses midnight)\n",
    "            if diff < 0:\n",
    "                diff += 24 * 60  # Add 24 hours worth of minutes\n",
    "                \n",
    "            return diff\n",
    "\n",
    "        features['actual_duration'] = features.apply(lambda row: time_difference_in_minutes(row['Deadline'], row['Created At']), axis=1)\n",
    "\n",
    "\n",
    "        features = features[features['actual_duration'] > 0]\n",
    "\n",
    "        # i want to calc time until deadline from diff between deadline and the time of the day\n",
    "        features[\"time_until_deadline\"] = features[\"Deadline\"].apply(lambda x: (x - datetime.now()).total_seconds() / 60)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # 2. Time-based Features\n",
    "\n",
    "\n",
    "        print(\" actual duration minutes \",features[\"actual_duration\"].head())\n",
    "        \n",
    "        # 3. User Workload Features\n",
    "        user_workload = workload_insights.groupby('User ID').agg({\n",
    "            'Total Tasks': 'mean',\n",
    "            'Completed Tasks': 'mean',\n",
    "            'Avg. Task Duration': 'mean',\n",
    "            'Productivity Score': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        features = features.merge(user_workload, on='User ID', how='left')\n",
    "       \n",
    "        \n",
    "        # 4. User Settings Features\n",
    "        integration_columns = ['Trello Linked', 'Email Linked']\n",
    "        for col in integration_columns:\n",
    "            user_settings[col] = (user_settings[col] == 'Yes').astype(int)\n",
    "        \n",
    "        user_settings['has_integrations'] = user_settings[integration_columns].sum(axis=1)\n",
    "        features = features.merge(\n",
    "            user_settings[['User ID', 'has_integrations']], \n",
    "            on='User ID', how='left'\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        # 5. Calendar Conflict Features\n",
    "        def get_calendar_conflicts(row, calendar_events):\n",
    "            user_events = calendar_events[calendar_events['User ID'] == row['User ID']]\n",
    "            deadline = row['Deadline']\n",
    "            conflicts = len(user_events[\n",
    "                (pd.to_datetime(user_events['Start Time']) <= deadline) & \n",
    "                (pd.to_datetime(user_events['End Time']) >= deadline)\n",
    "            ])\n",
    "            return conflicts\n",
    "        \n",
    "        features['calendar_conflicts'] = features.apply(\n",
    "            lambda x: get_calendar_conflicts(x, calendar_events), axis=1\n",
    "        )\n",
    "        \n",
    "        # 6. Reminder Features\n",
    "        reminders_per_user = reminders.groupby('User ID').size().reset_index(name='reminder_count')\n",
    "        features = features.merge(reminders_per_user, on='User ID', how='left')\n",
    "        features['reminder_count'] = features['reminder_count'].fillna(0)\n",
    "\n",
    "        if not hasattr(self.priority_encoder, 'classes_'): \n",
    "            features['priority_encoded'] = self.priority_encoder.fit_transform(features['Priority'])\n",
    "        else:\n",
    "            features['Priority'] = features['Priority'].apply(\n",
    "                lambda x: 'Medium' if x not in self.priority_encoder.classes_ else x\n",
    "            )   \n",
    "            features['priority_encoded'] = self.priority_encoder.transform(features['Priority'])\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        # # 7. Derived Task Features\n",
    "        # features['log_est_duration'] = np.log1p(features['Estimated Duration (min)'])\n",
    "        # features['sqrt_est_duration'] = np.sqrt(features['Estimated Duration (min)'])\n",
    "        # features['est_duration_per_productivity'] = features['Estimated Duration (min)'] / features['Productivity Score'].fillna(0.5)\n",
    "        # features['relative_duration'] = features['Estimated Duration (min)'] / features['Avg. Task Duration']\n",
    "        # features['duration_ratio'] = features['Estimated Duration (min)'] / features['time_until_deadline']\n",
    "       \n",
    "        # 8. Additional derived features\n",
    "        features['deadline_urgency'] = np.where(\n",
    "            features['time_until_deadline'] < 24, 3,\n",
    "            np.where(features['time_until_deadline'] < 72, 2, 1)\n",
    "        )\n",
    "        \n",
    "        features['workload_pressure'] = features['Total Tasks'] / features['Completed Tasks'].replace(0, 1)\n",
    "        features['normalized_workload'] = features['Total Tasks'] / features['Completed Tasks'].max()\n",
    "        features['completion_rate'] = features['Completed Tasks'] / features['Total Tasks'].replace(0, 1)\n",
    "        features['urgency_score'] = features['deadline_urgency'] * features['workload_pressure']\n",
    "        features['complexity_score'] = features['Estimated Duration (min)'] * features['calendar_conflicts']\n",
    "        features['time_pressure'] = features['Estimated Duration (min)'] / features['time_until_deadline']\n",
    "        features['workload_complexity'] = features['Total Tasks'] * features['Avg. Task Duration']\n",
    "        features['productivity_adjusted_duration'] = features['Estimated Duration (min)'] / features['Productivity Score'].fillna(0.5)\n",
    "        features['tasks_per_hour'] = features['Total Tasks'] / 24\n",
    "        features['available_time_ratio'] = features['time_until_deadline'] / features['Estimated Duration (min)']\n",
    "        features['weekend_workload'] = features['is_weekend'] * features['Total Tasks']\n",
    "        features['high_urgency'] = (features['deadline_urgency'] >= 2).astype(int)\n",
    "        features['critical_deadline'] = (features['time_until_deadline'] < 24).astype(int)\n",
    "        \n",
    "        # Drop original datetime columns after extracting features\n",
    "        # features = features.drop(columns=datetime_cols)\n",
    "        \n",
    "        # Handle missing values and infinities\n",
    "        features = features.replace([np.inf, -np.inf], np.nan)\n",
    "        numerical_columns = features.select_dtypes(include=['float64', 'int64']).columns\n",
    "        features[numerical_columns] = features[numerical_columns].fillna(features[numerical_columns].mean())\n",
    "        \n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        features[numerical_columns] = scaler.fit_transform(features[numerical_columns])\n",
    "\n",
    " \n",
    "        \n",
    "        # Verify no non-numeric data remains\n",
    "        non_numeric_cols = features.select_dtypes(exclude=['float64', 'int64','int32','datetime64[ns]']).columns\n",
    "        if len(non_numeric_cols) > 0:\n",
    "            print(f\"Warning: Non-numeric columns found: {non_numeric_cols}\")\n",
    "            features = features.select_dtypes(include=['float64', 'int64','int32','datetime64[ns]'])\n",
    "        \n",
    "        # Print feature names for debugging\n",
    "        print(\"\\nProcessed features:\")\n",
    "        for col in features.columns:\n",
    "            print(f\"- {col},{features[col].dtype}\")\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def train_models(self, data, calendar_events, reminders, event_schedule, user_settings, workload_insights):\n",
    "        \"\"\"Train models with comprehensive feature set and improved performance\"\"\"\n",
    "        prepared_data = self.Preprocessing_Features(data, calendar_events, reminders, \n",
    "                                            event_schedule, user_settings, workload_insights)\n",
    "\n",
    "        # Enhanced feature sets\n",
    "        duration_features = [\n",
    "            'time_until_deadline',\n",
    "            'Estimated Duration (min)',\n",
    "            'workload_pressure',\n",
    "            'Productivity Score'\n",
    "        ]\n",
    "\n",
    "        # Expanded priority features\n",
    "        priority_features = [\n",
    "            # Task-specific features\n",
    "            'Estimated Duration (min)', \n",
    "            # 'duration_ratio',\n",
    "            # 'log_est_duration',\n",
    "            # 'sqrt_est_duration',\n",
    "            # 'relative_duration',\n",
    "            \n",
    "            # Time-based features\n",
    "            'deadline_urgency',\n",
    "            'time_until_deadline',\n",
    "            \n",
    "            # Workload features\n",
    "            'Total Tasks',\n",
    "            'Completed Tasks',\n",
    "            'Avg. Task Duration',\n",
    "            'workload_pressure',\n",
    "            \n",
    "            # User-specific features\n",
    "            'Productivity Score',\n",
    "            'calendar_conflicts',\n",
    "            'reminder_count',\n",
    "            'has_integrations',\n",
    "            \n",
    "            # Derived features\n",
    "            'est_duration_per_productivity',\n",
    "            'duration_ratio',\n",
    "            \n",
    "            # Interaction features\n",
    "            'time_pressure',\n",
    "            'workload_complexity',\n",
    "            'productivity_adjusted_duration',\n",
    "            \n",
    "            # Time management features\n",
    "            'tasks_per_hour',\n",
    "            'available_time_ratio',\n",
    "            \n",
    "            # Weekend effect\n",
    "            'weekend_workload',\n",
    "            \n",
    "            # Urgency indicators\n",
    "            'high_urgency',\n",
    "            'critical_deadline'\n",
    "        ]\n",
    "            \n",
    "\n",
    "        # Store feature lists\n",
    "        self.duration_features = duration_features\n",
    "        self.priority_features = priority_features\n",
    "        \n",
    "        # Prepare feature matrices\n",
    "        X_duration = prepared_data[duration_features]\n",
    "        y_duration = prepared_data['actual_duration']\n",
    "        \n",
    "        X_priority = prepared_data[priority_features]\n",
    "        data['Priority'] = data['Priority'].map(self.priority_mapping)\n",
    "        y_priority = data['Priority']\n",
    "\n",
    "        \n",
    "        # Split data with stratification for priority\n",
    "        X_duration_train, X_duration_test, y_duration_train, y_duration_test = train_test_split(\n",
    "            X_duration, y_duration, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        X_priority_train, X_priority_test, y_priority_train, y_priority_test = train_test_split(\n",
    "            X_priority, y_priority, test_size=0.2, random_state=42, stratify=y_priority\n",
    "        )\n",
    "        \n",
    "        # Enhanced Duration Models with regularization\n",
    "        duration_models = {\n",
    "            'RandomForest': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=4,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        priority_models = {\n",
    "            'RandomForest': RandomForestClassifier(\n",
    "                n_estimators=500,\n",
    "                max_depth=5,  # Shallower to reduce overfitting\n",
    "                min_samples_split=10,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            ),\n",
    "            'XGBoost': XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate models with cross-validation\n",
    "        print(\"=== Duration Prediction Models ===\")\n",
    "        best_duration_model, best_duration_score, best_duration_name = self._train_regression_models(\n",
    "            duration_models, X_duration_train, y_duration_train, X_duration_test, y_duration_test\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Priority Classification Models ===\")\n",
    "        best_priority_model, best_priority_score, best_priority_name = self._train_classification_models(\n",
    "            priority_models, X_priority_train, y_priority_train, X_priority_test, y_priority_test\n",
    "        )\n",
    "        \n",
    "        # Store best models\n",
    "        self.duration_model = best_duration_model\n",
    "        self.priority_model = best_priority_model\n",
    "        self._is_fitted = True\n",
    "        \n",
    "        # Perform feature importance analysis\n",
    "        self._analyze_feature_importance(best_duration_model, duration_features, 'Duration')\n",
    "        self._analyze_feature_importance(best_priority_model, priority_features, 'Priority')\n",
    "        \n",
    "        # Learning curves analysis\n",
    "        self._plot_learning_curves(\n",
    "            best_duration_model, X_duration, y_duration, \n",
    "            f\"Learning Curves - {best_duration_name} (Duration Prediction)\",\n",
    "            'r2'\n",
    "        )\n",
    "        \n",
    "        self._plot_learning_curves(\n",
    "            best_priority_model, X_priority, y_priority,\n",
    "            f\"Learning Curves - {best_priority_name} (Priority Prediction)\",\n",
    "            'accuracy'\n",
    "        )\n",
    "        \n",
    "        return (best_duration_model, best_duration_score, best_duration_name,\n",
    "                best_priority_model, best_priority_score, best_priority_name)\n",
    "\n",
    "    def _analyze_feature_importance(self, model, feature_names, model_type):\n",
    "        \"\"\"Analyze and visualize feature importance\"\"\"\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            importances.sort_values(ascending=True).plot(kind='barh')\n",
    "            plt.title(f'Feature Importance for {model_type} Prediction')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nTop 5 Most Important Features for {model_type} Prediction:\")\n",
    "            print(importances.sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "    def analyze_model_performance(self, X, y_true, model, model_name, feature_names):\n",
    "        \"\"\"Comprehensive model performance analysis\"\"\"\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Basic metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"\\nModel Performance Analysis - {model_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"MSE: {mse:.2f}\")\n",
    "        print(f\"R2 Score: {r2:.2f}\")\n",
    "        print(f\"RMSE: {np.sqrt(mse):.2f}\")\n",
    "        \n",
    "        # Error analysis\n",
    "        errors = y_true - y_pred\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Error distribution\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(errors, bins=50)\n",
    "        plt.title('Error Distribution')\n",
    "        plt.xlabel('Prediction Error')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Actual vs Predicted\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "        plt.title('Actual vs Predicted')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict_task_duration(self, task, calendar_events, reminders, event_schedule, user_settings, workload_insights):\n",
    "        \"\"\"Predict task duration with enhanced features\"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "            \n",
    "        # Prepare single task features\n",
    "        task_df = pd.DataFrame([task])\n",
    "        prepared_features = self._prepare_features(task_df, calendar_events, reminders,\n",
    "                                                 event_schedule, user_settings, workload_insights)\n",
    "        \n",
    "        X = prepared_features[self.duration_features]\n",
    "        return float(self.duration_model.predict(X)[0])\n",
    "\n",
    "\n",
    "    def predict_task_priority(self, task, calendar_events, reminders, event_schedule, user_settings, workload_insights):\n",
    "        \"\"\"Predict task priority with enhanced features\"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "            \n",
    "        # Prepare single task features\n",
    "        task_df = pd.DataFrame([task])\n",
    "        prepared_features = self._prepare_features(\n",
    "            task_df,\n",
    "            calendar_events,\n",
    "            reminders,\n",
    "            event_schedule,\n",
    "            user_settings,\n",
    "            workload_insights\n",
    "        )\n",
    "        \n",
    "        X = prepared_features[self.priority_features]\n",
    "        priority_pred = self.priority_model.predict(X)[0]\n",
    "        \n",
    "        # Return numerical value instead of string\n",
    "        return self.priority_mapping.get(\n",
    "            self.priority_encoder.inverse_transform([priority_pred])[0], 2\n",
    "        )  # Default to Medium (2) if mapping fails\n",
    "\n",
    "    def _train_regression_models(self, models, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train and evaluate regression models\"\"\"\n",
    "        best_model = None\n",
    "        best_score = float('-inf')\n",
    "        best_model_name = \"\"\n",
    "        \n",
    "        print(\"\\nDuration Model Performance:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                    cv=5, scoring='r2')\n",
    "            cv_r2 = cv_scores.mean()\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  MSE: {mse:.2f}\")\n",
    "            print(f\"  R2: {r2:.2f}\")\n",
    "            print(f\"  CV R2: {cv_r2:.2f} (±{cv_scores.std()*2:.2f})\")\n",
    "            \n",
    "            # Track best model\n",
    "            if r2 > best_score:\n",
    "                best_score = r2\n",
    "                best_model = model\n",
    "                best_model_name = name\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Best Duration Model: {best_model_name}\")\n",
    "        print(f\"Best R2 Score: {best_score:.2f}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return best_model, best_score, best_model_name\n",
    "    \n",
    "\n",
    "    def _train_classification_models(self, models, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train and evaluate multiple classification models with hyperparameter tuning\"\"\"\n",
    "        \n",
    "        # Initialize models dictionary with more classifiers\n",
    "        models = {\n",
    "            'RandomForest': RandomForestClassifier(random_state=42),\n",
    "            'XGBoost': XGBClassifier(random_state=42),\n",
    "            'LightGBM': LGBMClassifier(random_state=42),\n",
    "            'SVM': SVC(random_state=42),\n",
    "            'LogisticRegression': LogisticRegression(random_state=42),\n",
    "            'KNeighbors': KNeighborsClassifier()\n",
    "        }\n",
    "\n",
    "        # Define parameter grids for each model\n",
    "        param_grids = {\n",
    "            'RandomForest': {\n",
    "                'n_estimators': [300, 500],\n",
    "                'max_depth': [5, 7, None],\n",
    "                'min_samples_split': [5, 10],\n",
    "                'min_samples_leaf': [2, 4],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [300, 500],\n",
    "                'max_depth': [5, 7],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'subsample': [0.8, 1.0],\n",
    "                'colsample_bytree': [0.8, 1.0]\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': [300, 500],\n",
    "                'max_depth': [5, 7, -1],\n",
    "                'learning_rate': [0.01, 0.1],\n",
    "                'num_leaves': [31, 127],\n",
    "                'subsample': [0.8, 1.0]\n",
    "            },\n",
    "            'SVM': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['rbf', 'linear'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            },\n",
    "            'LogisticRegression': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            },\n",
    "            'KNeighbors': {\n",
    "                'n_neighbors': [3, 5, 7],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        best_model = None\n",
    "        best_score = 0\n",
    "        best_model_name = \"\"\n",
    "        results_dict = {}\n",
    "\n",
    "        # Convert X_train and X_test to numpy arrays if they're DataFrames\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            X_train = X_train.values\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test = X_test.values\n",
    "\n",
    "        # Scale features for specific algorithms\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Training {name} model...\")\n",
    "            \n",
    "            # Use scaled data for specific algorithms\n",
    "            if name in ['SVM', 'LogisticRegression', 'KNeighbors']:\n",
    "                X_train_use = X_train_scaled\n",
    "                X_test_use = X_test_scaled\n",
    "            else:\n",
    "                X_train_use = X_train\n",
    "                X_test_use = X_test\n",
    "\n",
    "            try:\n",
    "                # Create GridSearchCV object\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=param_grids[name],\n",
    "                    scoring=['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted'],\n",
    "                    refit='f1_weighted',\n",
    "                    cv=5,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "\n",
    "                # Fit GridSearchCV\n",
    "                grid_search.fit(X_train_use, y_train)\n",
    "\n",
    "                # Get best model and predictions\n",
    "                best_params = grid_search.best_params_\n",
    "                best_estimator = grid_search.best_estimator_\n",
    "                y_pred = best_estimator.predict(X_test_use)\n",
    "\n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                # Store results\n",
    "                results_dict[name] = {\n",
    "                    'model': best_estimator,\n",
    "                    'best_params': best_params,\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_score': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall\n",
    "                }\n",
    "\n",
    "                # Print results\n",
    "                print(f\"\\n{name} Results:\")\n",
    "                print(\"Best Parameters:\", best_params)\n",
    "                print(f\"Best CV Score (F1): {grid_search.best_score_:.3f}\")\n",
    "                print(f\"Test Set Metrics:\")\n",
    "                print(f\"  Accuracy:  {accuracy:.3f}\")\n",
    "                print(f\"  F1 Score:  {f1:.3f}\")\n",
    "                print(f\"  Precision: {precision:.3f}\")\n",
    "                print(f\"  Recall:    {recall:.3f}\")\n",
    "\n",
    "                # Print classification report\n",
    "                print(\"\\nClassification Report:\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "\n",
    "                # Plot confusion matrix\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                cm = confusion_matrix(y_test, y_pred)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "                plt.title(f'Confusion Matrix - {name}')\n",
    "                plt.ylabel('True Label')\n",
    "                plt.xlabel('Predicted Label')\n",
    "                plt.show()\n",
    "\n",
    "                # Plot ROC curve if applicable\n",
    "                if hasattr(best_estimator, \"predict_proba\"):\n",
    "                    RocCurveDisplay.from_estimator(best_estimator, X_test_use, y_test)\n",
    "                    plt.title(f'ROC Curve - {name}')\n",
    "                    plt.show()\n",
    "\n",
    "                # Track best model\n",
    "                if f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_model = best_estimator\n",
    "                    best_model_name = name\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name} model: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if best_model is None:\n",
    "            raise ValueError(\"No models were successfully trained\")\n",
    "\n",
    "        # Print comparative analysis\n",
    "        print(\"\\nComparative Analysis of All Models:\")\n",
    "        comparison_df = pd.DataFrame({\n",
    "            model_name: {\n",
    "                'Accuracy': results['accuracy'],\n",
    "                'F1 Score': results['f1_score'],\n",
    "                'Precision': results['precision'],\n",
    "                'Recall': results['recall']\n",
    "            }\n",
    "            for model_name, results in results_dict.items()\n",
    "        }).T\n",
    "\n",
    "        print(\"\\nModel Performance Comparison:\")\n",
    "        print(comparison_df)\n",
    "\n",
    "        # Plot comparative bar chart\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        comparison_df.plot(kind='bar', width=0.8)\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Best Model: {best_model_name}\")\n",
    "        print(f\"Best F1 Score: {best_score:.3f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        return best_model, best_score, best_model_name\n",
    "\n",
    "    def plot_roc_curve(model, X_test, y_test):\n",
    "        \"\"\"Plot ROC curve for multi-class classification\"\"\"\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        n_classes = len(np.unique(y_test))\n",
    "        \n",
    "        # Plot ROC curve for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test == i, y_pred_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        colors = cycle(['blue', 'red', 'green'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                    label=f'ROC curve of class {i} (area = {roc_auc[i]:0.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # def _train_classification_models(self, models, X_train, y_train, X_test, y_test):\n",
    "    #     \"\"\"Train and evaluate classification models\"\"\"\n",
    "    #     best_model = None\n",
    "    #     best_score = 0\n",
    "    #     best_model_name = \"\"\n",
    "        \n",
    "    #     for name, model in models.items():\n",
    "    #         # Train model\n",
    "    #         model.fit(X_train, y_train)\n",
    "            \n",
    "    #         # Evaluate\n",
    "    #         y_pred = model.predict(X_test)\n",
    "    #         accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "    #         # Cross-validation\n",
    "    #         cv_scores = cross_val_score(model, X_train, y_train, \n",
    "    #                                 cv=5, scoring='accuracy')\n",
    "    #         cv_accuracy = cv_scores.mean()\n",
    "    #         # Add F1 score evaluation\n",
    "    #         f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    #         print(f\"  F1 Score: {f1:.2f}\")\n",
    "            \n",
    "    #         # Add confusion matrix\n",
    "    #         print(\"  Confusion Matrix:\")\n",
    "    #         print(confusion_matrix(y_test, y_pred))\n",
    "            \n",
    "    #         print(f\"\\n{name}:\")\n",
    "    #         print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    #         print(f\"  CV Accuracy: {cv_accuracy:.2f} (±{cv_scores.std()*2:.2f})\")\n",
    "    #         print(\"  Classification Report:\")\n",
    "    #         print(classification_report(y_test, y_pred))\n",
    "            \n",
    "    #         # Print class distribution\n",
    "    #         unique, counts = np.unique(y_train, return_counts=True)\n",
    "    #         print(\"  Class Distribution in Training Data:\")\n",
    "    #         for u, c in zip(unique, counts):\n",
    "    #             priority_label = self.priority_encoder.classes_[u]\n",
    "    #             print(f\"    Class {u} (Priority {priority_label}): {c} samples\")\n",
    "            \n",
    "    #         # Track best model\n",
    "    #         if accuracy > best_score:\n",
    "    #             best_score = accuracy\n",
    "    #             best_model = model\n",
    "    #             best_model_name = name\n",
    "        \n",
    "    #     print(\"\\n\" + \"=\" * 50)\n",
    "    #     print(f\"Best Priority Model: {best_model_name}\")\n",
    "    #     print(f\"Best Accuracy Score: {best_score:.2f}\")\n",
    "    #     print(\"=\" * 50)\n",
    "        \n",
    "    #     return best_model, best_score, best_model_name\n",
    "    \n",
    "    def _plot_learning_curves(self, model, X, y, title, scoring):\n",
    "        \"\"\"Plot learning curves to analyze model performance\"\"\"\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            cv=5,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_mean, label='Training score')\n",
    "        plt.plot(train_sizes, val_mean, label='Cross-validation score')\n",
    "        \n",
    "        plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "        plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('Training Examples')\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your datasets (replace with your actual file paths)\n",
    "calendar_events = pd.read_csv(\"calendar_events.csv\")\n",
    "reminders = pd.read_csv(\"reminders_followups.csv\")\n",
    "event_schedule = pd.read_csv(\"event_schedule.csv\")\n",
    "user_settings = pd.read_csv(\"user_settings.csv\")\n",
    "workload_insights = pd.read_csv(\"workload_insights.csv\")\n",
    "user_tasks = pd.read_csv(\"user_tasks.csv\")\n",
    "\n",
    "# Prepare training data\n",
    "ml_training_data = user_tasks[['User ID', 'Priority', 'Estimated Duration (min)', 'Deadline', 'Created At', 'Status']].copy()\n",
    "\n",
    "# def convert_minutes_to_datetime(minutes_str):\n",
    "#     try:\n",
    "#         # Split the string and convert to float\n",
    "#         total_minutes, seconds = map(float, minutes_str.split(':'))\n",
    "        \n",
    "#         # Convert total minutes to hours and minutes\n",
    "#         hours = int(total_minutes // 60)\n",
    "#         minutes = int(total_minutes % 60)\n",
    "        \n",
    "#         # Combine with seconds\n",
    "#         time_str = f\"{hours:02}:{minutes:02}:{int(seconds):02}\"\n",
    "        \n",
    "#         # Convert to datetime\n",
    "#         return pd.to_datetime(time_str, format='%H:%M:%S')\n",
    "#     except:\n",
    "#         # If conversion fails, return NaT\n",
    "#         return pd.NaT\n",
    "\n",
    "\n",
    "\n",
    "# # Convert 'Deadline' and 'Created At' columns to datetime\n",
    "# ml_training_data['Deadline'] = ml_training_data['Deadline'].apply(convert_minutes_to_datetime)\n",
    "# ml_training_data['Created At'] = ml_training_data['Created At'].apply(convert_minutes_to_datetime)\n",
    "\n",
    "# ml_training_data['Deadline'] = ml_training_data['Deadline'].dt.time\n",
    "# ml_training_data['Created At'] = ml_training_data['Created At'].dt.time\n",
    "\n",
    "\n",
    "# # Calculate 'actual_duration' in minutes\n",
    "# ml_training_data['actual_duration'] = (ml_training_data['Deadline'] - ml_training_data['Created At']).dt.total_seconds() / 60\n",
    "\n",
    "# # Clean data\n",
    "# ml_training_data = ml_training_data.dropna(subset=['actual_duration', 'Priority', 'Estimated Duration (min)'])\n",
    "# ml_training_data = ml_training_data[ml_training_data['actual_duration'] > 0]\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "ml_model = EnhancedMLModel()\n",
    "\n",
    "# Train models and get results\n",
    "(best_duration_model, \n",
    "best_duration_score, \n",
    "best_duration_name,\n",
    "best_priority_model,\n",
    "best_priority_score,\n",
    "best_priority_name) = ml_model.train_models(\n",
    "ml_training_data,\n",
    "calendar_events,\n",
    "reminders,\n",
    "event_schedule,\n",
    "user_settings,\n",
    "workload_insights\n",
    ")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Final Model Selection Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest Duration Prediction Model: {best_duration_name}\")\n",
    "print(f\"R2 Score: {best_duration_score:.4f}\")\n",
    "print(f\"\\nBest Priority Classification Model: {best_priority_name}\")\n",
    "print(f\"Accuracy Score: {best_priority_score:.4f}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Store the trained models for later use\n",
    "ml_model.duration_model = best_duration_model\n",
    "ml_model.priority_model = best_priority_model\n",
    "ml_model._is_fitted = True\n",
    "\n",
    "# # Example of making predictions with the trained models\n",
    "# sample_task = {\n",
    "#     'User ID': user_settings['User ID'].iloc[0],\n",
    "#     'Priority': 'High',\n",
    "#     'Estimated Duration (min)': 120,\n",
    "#     'Deadline': pd.Timestamp.now() + pd.Timedelta(hours=2),\n",
    "#     'Created At': pd.Timestamp.now(),\n",
    "#     'Status': 'Pending'\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     predicted_duration = ml_model.predict_task_duration(\n",
    "#         sample_task,\n",
    "#         calendar_events,\n",
    "#         reminders,\n",
    "#         event_schedule,\n",
    "#         user_settings,\n",
    "#         workload_insights\n",
    "#     )\n",
    "#     print(f\"\\nSample Task Duration Prediction: {predicted_duration:.2f} minutes\")\n",
    "\n",
    "#     predicted_priority = ml_model.predict_task_priority(\n",
    "#         sample_task,\n",
    "#         calendar_events,\n",
    "#         reminders,\n",
    "#         event_schedule,\n",
    "#         user_settings,\n",
    "#         workload_insights\n",
    "#     )\n",
    "#     print(f\"Sample Task Priority Prediction: {ml_model.priority_reverse_mapping[predicted_priority]}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nPrediction error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
