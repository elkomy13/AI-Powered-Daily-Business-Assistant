{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import spacy\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier  # Added LightGBM imports\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Class distribution in training data:\n",
      "priority_encoded\n",
      "0    1379\n",
      "2    1322\n",
      "1    1299\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       276\n",
      "           1       1.00      1.00      1.00       260\n",
      "           2       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00       800\n",
      "   macro avg       1.00      1.00      1.00       800\n",
      "weighted avg       1.00      1.00      1.00       800\n",
      "\n",
      "Prediction distribution:\n",
      "0    276\n",
      "2    264\n",
      "1    260\n",
      "Name: count, dtype: int64\n",
      "Using board: My Trello board\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: Conduct team meeting to discuss next sprint goals\n",
      "Description: Schedule and prepare agenda for sprint planning\n",
      "Predicted Priority: Medium\n",
      "Categories: ['General']\n",
      "Created Trello card with ID: 6803ec274499460e33120b53\n",
      "Set Medium priority reminder for card 6803ec274499460e33120b53 due at 2025-04-20T20:32:07.659885\n",
      "\n",
      "Processing task: Fix critical bug in payment processing\n",
      "Description: Customers reporting payment failures in checkout\n",
      "Predicted Priority: Medium\n",
      "Categories: ['Analytical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trello card with ID: 6803ec28e7f040e22e668940\n",
      "Set Medium priority reminder for card 6803ec28e7f040e22e668940 due at 2025-04-20T20:32:08.436147\n",
      "\n",
      "Processing task: Update documentation for new API endpoints\n",
      "Description: Document the 3 new endpoints added last week\n",
      "Predicted Priority: Medium\n",
      "Categories: ['Content-Creation', 'Time-Sensitive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trello card with ID: 6803ec29c940cc6b7349a435\n",
      "Set Medium priority reminder for card 6803ec29c940cc6b7349a435 due at 2025-04-20T20:32:09.223689\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "class TrelloIntegrationManager:\n",
    "    def __init__(self, api_key, token):\n",
    "        self.api_key = api_key\n",
    "        self.token = token\n",
    "        self.base_url = 'https://api.trello.com/1'\n",
    "\n",
    "    def _make_request(self, method, endpoint, params=None, data=None):\n",
    "        \"\"\"Generic method to make API requests\"\"\"\n",
    "        url = f'{self.base_url}/{endpoint}'\n",
    "        \n",
    "        # Always include key and token in params\n",
    "        request_params = {\n",
    "            'key': self.api_key,\n",
    "            'token': self.token\n",
    "        }\n",
    "        \n",
    "        if params:\n",
    "            request_params.update(params)\n",
    "        \n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, params=request_params)\n",
    "            elif method == 'POST':\n",
    "                response = requests.post(url, params=request_params, json=data)\n",
    "            elif method == 'PUT':\n",
    "                response = requests.put(url, params=request_params, json=data)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported HTTP method: {method}\")\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Trello API Request Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_card(self, list_id, name, description=None, due_date=None):\n",
    "        \"\"\"Create a new card in a specific list\"\"\"\n",
    "        endpoint = 'cards'\n",
    "        data = {\n",
    "            'idList': str(list_id),\n",
    "            'name': str(name),\n",
    "            'desc': str(description or '')\n",
    "        }\n",
    "        \n",
    "        if due_date:\n",
    "            # Ensure due_date is a string in ISO format\n",
    "            if isinstance(due_date, (datetime)):\n",
    "                due_date = due_date.isoformat()\n",
    "            data['due'] = str(due_date)\n",
    "        \n",
    "        return self._make_request('POST', endpoint, data=data)\n",
    "\n",
    "    def update_card_due_date(self, card_id, due_date):\n",
    "        \"\"\"Update a card's due date\"\"\"\n",
    "        endpoint = f'cards/{card_id}'\n",
    "        data = {'due': due_date}\n",
    "        return self._make_request('PUT', endpoint, data=data)\n",
    "\n",
    "    def get_boards(self):\n",
    "        \"\"\"Retrieve all boards for the authenticated user\"\"\"\n",
    "        endpoint = 'members/me/boards'\n",
    "        params = {\n",
    "            'fields': 'name,url,id',\n",
    "            'lists': 'open'\n",
    "        }\n",
    "        return self._make_request('GET', endpoint, params)\n",
    "\n",
    "    def get_lists(self, board_id):\n",
    "        \"\"\"Get lists for a specific board\"\"\"\n",
    "        endpoint = f'boards/{board_id}/lists'\n",
    "        params = {\n",
    "            'fields': 'name,id'\n",
    "        }\n",
    "        return self._make_request('GET', endpoint, params)\n",
    "\n",
    "class PriorityBasedTaskManager:\n",
    "    def __init__(self):\n",
    "        # Priority mapping (Low:0, Medium:1, High:2)\n",
    "        self.priority_mapping = {'Low': 2, 'Medium': 1, 'High': 0}\n",
    "        self.priority_reverse_mapping = {2: 'Low', 1: 'Medium', 0: 'High'}\n",
    "        self.priority_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.nlp_processor = NLPProcessor()\n",
    "        \n",
    "    def preprocess_data(self, data):\n",
    "        \"\"\"Prepare data for modeling focusing only on priority\"\"\"\n",
    "        features = data.copy()\n",
    "        \n",
    "        # Ensure priority is categorical\n",
    "        features['Priority'] = features['Priority'].astype(str)\n",
    "        \n",
    "        # Encode priority if not already done\n",
    "        if not hasattr(self.priority_encoder, 'classes_'):\n",
    "            features['priority_encoded'] = self.priority_encoder.fit_transform(features['Priority'])\n",
    "        else:\n",
    "            # Handle new priority values not seen during training\n",
    "            features['Priority'] = features['Priority'].apply(\n",
    "                lambda x: 'Medium' if x not in self.priority_encoder.classes_ else x\n",
    "            )\n",
    "            features['priority_encoded'] = self.priority_encoder.transform(features['Priority'])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train_model(self, data):\n",
    "        \"\"\"Train a priority prediction model\"\"\"\n",
    "        prepared_data = self.preprocess_data(data)\n",
    "        \n",
    "        # Use NLP to extract features from task description\n",
    "        text_features = self.nlp_processor.extract_text_features(data['Description'])\n",
    "        \n",
    "        # Combine features\n",
    "        X = pd.concat([\n",
    "            pd.DataFrame(text_features),\n",
    "            prepared_data[['priority_encoded']]  # Current priority as feature\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Convert all column names to strings to avoid the TypeError\n",
    "        X.columns = X.columns.astype(str)\n",
    "        \n",
    "        y = prepared_data['priority_encoded']\n",
    "        \n",
    "        # Print class distribution before splitting\n",
    "        print(\"Class distribution in training data:\")\n",
    "        print(pd.Series(y).value_counts())\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Simple model focusing on priority\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Add prediction distribution check\n",
    "        print(\"Prediction distribution:\")\n",
    "        print(pd.Series(y_pred).value_counts())\n",
    "\n",
    "        return self.model\n",
    "        \n",
    "    def predict_priority(self, task_description, current_priority='Medium'):\n",
    "        \"\"\"Predict task priority based on description and current priority\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "            \n",
    "        # Process text features\n",
    "        text_features = self.nlp_processor.extract_text_features([task_description])\n",
    "        \n",
    "        # Prepare input data\n",
    "        input_data = pd.DataFrame(text_features)\n",
    "        input_data['priority_encoded'] = self.priority_mapping.get(current_priority, 1)  # Default to Medium\n",
    "        \n",
    "        # Convert all column names to strings to match training data\n",
    "        input_data.columns = input_data.columns.astype(str)\n",
    "        \n",
    "        # Predict\n",
    "        priority_num = self.model.predict(input_data)[0]\n",
    "        return self.priority_reverse_mapping[priority_num]\n",
    "    \n",
    "    def categorize_task(self, description):\n",
    "        \"\"\"Categorize task using NLP\"\"\"\n",
    "        return self.nlp_processor.categorize_task(description)\n",
    "\n",
    "class NLPProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.text_vectorizer = TfidfVectorizer(max_features=100)\n",
    "        \n",
    "    def extract_text_features(self, descriptions):\n",
    "        \"\"\"Extract NLP-based features from task descriptions\"\"\"\n",
    "        # Process descriptions with spaCy\n",
    "        processed_texts = [\" \".join([token.lemma_ for token in self.nlp(doc)]) for doc in descriptions]\n",
    "        \n",
    "        # Vectorize text\n",
    "        if not hasattr(self.text_vectorizer, 'vocabulary_'):\n",
    "            text_features = self.text_vectorizer.fit_transform(processed_texts)\n",
    "        else:\n",
    "            text_features = self.text_vectorizer.transform(processed_texts)\n",
    "            \n",
    "        return pd.DataFrame.sparse.from_spmatrix(text_features)\n",
    "    \n",
    "    def categorize_task(self, description):\n",
    "        \"\"\"Categorize task based on description content\"\"\"\n",
    "        doc = self.nlp(description)\n",
    "        categories = set()\n",
    "        \n",
    "        # Entity-based categorization\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\":\n",
    "                categories.add(\"Organization\")\n",
    "            elif ent.label_ == \"PERSON\":\n",
    "                categories.add(\"Person-Related\")\n",
    "            elif ent.label_ == \"DATE\":\n",
    "                categories.add(\"Time-Sensitive\")\n",
    "            elif ent.label_ == \"MONEY\":\n",
    "                categories.add(\"Financial\")\n",
    "        \n",
    "        # Verb-based categorization\n",
    "        if any(token.lemma_ in [\"report\", \"analyze\", \"research\"] for token in doc):\n",
    "            categories.add(\"Analytical\")\n",
    "            \n",
    "        if any(token.lemma_ in [\"meeting\", \"call\", \"discuss\"] for token in doc):\n",
    "            categories.add(\"Communication\")\n",
    "            \n",
    "        if any(token.lemma_ in [\"buy\", \"purchase\", \"order\"] for token in doc):\n",
    "            categories.add(\"Procurement\")\n",
    "            \n",
    "        if any(token.lemma_ in [\"write\", \"document\", \"create\"] for token in doc):\n",
    "            categories.add(\"Content-Creation\")\n",
    "            \n",
    "        return list(categories) if categories else [\"General\"]\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Trello API credentials\n",
    "    TRELLO_API_KEY = '9bd35201acaf3e9f629af9c7648b6e26'\n",
    "    TRELLO_TOKEN = 'ATTAd2ea9551043c1590ec93062a5df503dfa2b5c5b483fdf18b93c8ab176b981499380E20EE'\n",
    "\n",
    "    # Load sample data\n",
    "    print(\"Loading datasets...\")\n",
    "    user_tasks = pd.read_csv(\"modified_user_tasks.csv\")\n",
    "    ml_training_data = user_tasks[['Description', 'Priority']].copy()\n",
    "\n",
    "    # Initialize and train the system\n",
    "    task_manager = PriorityBasedTaskManager()\n",
    "    task_manager.train_model(ml_training_data)\n",
    "\n",
    "    # Initialize Trello integration\n",
    "    trello_manager = TrelloIntegrationManager(TRELLO_API_KEY, TRELLO_TOKEN)\n",
    "\n",
    "    # Get the first board (or create one if none exists)\n",
    "    boards = trello_manager.get_boards()\n",
    "    if not boards:\n",
    "        print(\"No boards found, please create one in Trello first\")\n",
    "        exit()\n",
    "        \n",
    "    sample_board = boards[0]\n",
    "    print(f\"Using board: {sample_board['name']}\")\n",
    "\n",
    "    # Get lists from the board\n",
    "    lists = trello_manager.get_lists(sample_board['id'])\n",
    "    if not lists:\n",
    "        print(\"No lists found in the board\")\n",
    "        exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: Resolve server outage affecting customer access\n",
      "Description: Urgent server issue preventing customers from accessing services\n",
      "Actual Priority: High\n",
      "Predicted Priority: High\n",
      "Categories: ['General']\n",
      "Created Trello card with ID: 6803ed165531c1aaa1500471\n",
      "Set High priority reminder for card 6803ed165531c1aaa1500471 due at 2025-04-19T22:36:07.064552\n",
      "\n",
      "Processing task: Conduct team meeting to discuss next sprint goals\n",
      "Description: Schedule and prepare agenda for sprint planning\n",
      "Actual Priority: Medium\n",
      "Predicted Priority: Medium\n",
      "Categories: ['General']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trello card with ID: 6803ed170a7ae679867db21c\n",
      "Set Medium priority reminder for card 6803ed170a7ae679867db21c due at 2025-04-20T20:36:07.865838\n",
      "\n",
      "Processing task: Reply to non-urgent emails and notifications\n",
      "Description: Respond to general inquiries and notifications\n",
      "Actual Priority: Low\n",
      "Predicted Priority: Medium\n",
      "Categories: ['General']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youss\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Trello card with ID: 6803ed187162f86c9cee8fb6\n",
      "Set Medium priority reminder for card 6803ed187162f86c9cee8fb6 due at 2025-04-20T20:36:08.636211\n"
     ]
    }
   ],
   "source": [
    "# Sample tasks to test with actual priorities\n",
    "sample_tasks = [\n",
    "    {\"name\": \"Resolve server outage affecting customer access\", \n",
    "     \"desc\": \"Urgent server issue preventing customers from accessing services\",\n",
    "     \"actual_priority\": \"High\"},\n",
    "    \n",
    "    {\"name\": \"Conduct team meeting to discuss next sprint goals\", \n",
    "     \"desc\": \"Schedule and prepare agenda for sprint planning\",\n",
    "     \"actual_priority\": \"Medium\"},\n",
    "    \n",
    "    {\"name\": \"Reply to non-urgent emails and notifications\", \n",
    "     \"desc\": \"Respond to general inquiries and notifications\",\n",
    "     \"actual_priority\": \"Low\"}\n",
    "]\n",
    "\n",
    "# Add sample tasks to Trello and set reminders\n",
    "for task in sample_tasks:\n",
    "    # Predict priority\n",
    "    predicted_priority = task_manager.predict_priority(task['desc'])\n",
    "    categories = task_manager.categorize_task(task['desc'])\n",
    "    \n",
    "    print(f\"\\nProcessing task: {task['name']}\")\n",
    "    print(f\"Description: {task['desc']}\")\n",
    "    print(f\"Actual Priority: {task['actual_priority']}\")\n",
    "    print(f\"Predicted Priority: {predicted_priority}\")\n",
    "    print(f\"Categories: {categories}\")\n",
    "    \n",
    "    # Create card in Trello (using first list for simplicity)\n",
    "    card = trello_manager.create_card(\n",
    "        list_id=lists[0]['id'],\n",
    "        name=task['name'],\n",
    "        description=f\"{task['desc']}\\n\\nActual Priority: {task['actual_priority']}\\nPredicted Priority: {predicted_priority}\",\n",
    "        due_date=None  # Will be set based on priority\n",
    "    )\n",
    "    \n",
    "    if card:\n",
    "        print(f\"Created Trello card with ID: {card['id']}\")\n",
    "        set_trello_reminders(card['id'], predicted_priority)\n",
    "    else:\n",
    "        print(\"Failed to create Trello card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
